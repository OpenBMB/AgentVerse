cnt_critic_agents: 3


max_loop_rounds: 4


max_criticizing_rounds: 1


human_eval: false


task_description: |-
  AgentVerse is a open-source team devoted to developing LLM agent platform for accomplishing tasks in the real world. Generate a proposal about 3-day employee orientation program tailored for newly hired engineers at AgentVerse. The proposal, including targets and plans, does not exceed 50 words.
  

evaluation_dimensions: |-
  


prompts:
  role_assigner_prompt: &role_assigner_prompt |-
    You are the leader of a group of experts, now you are faced with a task:
    
    ${task_description}
    
    You can recruit ${cnt_critic_agents} expert team members in different regions.
    What experts will you recruit to better generate good ideas?
    
    Output format example:
    1. an electrical engineer specified in the filed of xxx
    2. an economist who is good at xxx
    3. a lawyer with a good knowledge of xxx
    ...
    
    ${advice}
    You don't have to give the reason.

  solver_prompt: &solver_prompt |-
    You are faced with the task:
    ${task_description}

    You are going to solve the task step by step, and you already gave the following solution:
    ${former_solution}
    
    Other experts in your group gave the following opinions:
    ${critic_opinions}
    
    Now you are going to give a new step of solution to that task, based upon your former solution, the experts opinions and task goal:
    
    (If no critic opinions are given, you can just give new solutions freely)
    (You can just generate a new solution from step 1 if no former solution is given)
    (Remember, only give one point of solution in one round)
    
    Output format: (Step n): (Your solution in one line), where n is the number of the step, starting from 1 and increasing by 1 each round.

  summarizer_prompt: &summarizer_prompt |-
    You are a summarizer. 
    Your task is to categorize and summarize the ideas in the chat history.
    Please add the speaker of each idea to the beginning of the content.


    Your goal is to give the best solution to the following task ${task_description}.
    
    #Output format
    1. (Speaker1): (Ideas of Speaker 1 in a single line)
    2. (Speaker2): (Ideas of Speaker 2 in a single line)
    3. (Speaker3): (Ideas of Speaker 3 in a single line)
    ...
    
    Here is the content you have to summarize:
    ${former_solution}
    ${critic_opinions}

    Please merge all ideas of one speaker into one item.

  critic_prompt: &critic_prompt |-
    Now you are ${role_description}
    
    You are in a discussion group, aiming to ${task_description}.
    
    Now the group is going to give a preliminary solution as the following:

    ${preliminary_solution}
    
    Now the group is asking your opinion about it. Based on your knowledge
    in your field, do you agree that this solution can perfectly 
    solve the problem?
    Or do you have any ideas to improve it?
    
    - If you thinks it is perfect, use the following output format:
    Action: Agree
    Action Input: Agree.
    (Do not output your reason for agreeing!)

    - If you want to give complemented opinions to improve it or to contradict with it, use the following output format:
    Action: Disagree
    Action Input: (what you want to say in one line)
    
    P.S. Always remember you are ${role_description}!
    
    ${advice}
    If no former solution or critic opinions are given, you can just disagree and output your idea freely, based on the expertise of your role.
    Remember, the ideas should be specific and detailed enough, not just general opinions.

  evaluator_prompt: &evaluator_prompt |-
    Your task is to evaluate the ideas in the solution.
 
    The goal is to ${task_description}.
    
    Please rate the ideas in the content in the following dimensions:
        1. Comprehensiveness:Are they comprehensive enough to cover all the 
           important aspects a engineering project may have?
        2. Detailedness: Are they detailed enough to be implemented?
        3. Feasibility: Are they reasonable and practical?
        4. Novelty: Are they creative and innovative?
    
    0 means the idea is like random generated ideas,
    10 means the idea is perfect in that aspect.
    
    and then in the fifth line of output, give your detailed advice for the solution generators.
    You can also give advice to the human resource staff on what experts they should recruit.
    Just say the drawbacks of the ideas, no need to do compliments first.
    
  
    #Output format
    You must output in the following format:
    1. Comprehensiveness: (a score between 0 and 9)
    2. Detailedness: (a score between 0 and 9)
    3. Feasibility: (a score between 0 and 9)
    4. Novelty: (a score between 0 and 9)
    5. Advice: (your advice in one line)
    
    Here is the content you have to evaluate:
    ${solution}
    

name: pipeline


environment:
  env_type: task-basic
  max_loop_rounds: 3
  rule:
    order:
      type: sequential
    visibility:
      type: all
    selector:
      type: basic
    updater:
      type: basic
    describer:
      type: basic

agents:
  - #role_assigner_agent:
    agent_type: role_assigner
    name: role assigner
    prompt_template: *role_assigner_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 0.5
      max_tokens: 256
    output_parser:
      type: role_assigner

  - #solver_agent:
    agent_type: solver
    name: Planner
    prompt_template: [*solver_prompt, *summarizer_prompt]
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 0.7
      max_tokens: 512

  - #critic_agents:
    agent_type: critic
    name: Critic 1
    role_description: |-
      Waiting to be assigned.
    prompt_template: *critic_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 1.0
      max_tokens: 256
    output_parser:
      type: critic

  - #executor_agent:
    agent_type: executor
    name: Executor
    prompt_template: None
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 0.7
      max_tokens: 512

  - #evaluator_agent:
    agent_type: evaluator
    name: Evaluator
    role_description: |-
      Evaluator
    prompt_template: *evaluator_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 0.7
      max_tokens: 128
    output_parser:
      type: evaluator
      dimensions:
        - Comprehensiveness
        - Detailedness
        - Feasibility
        - Novelty


tools:

